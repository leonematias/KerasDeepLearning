{"cells":[{"metadata":{"trusted":true,"_uuid":"6fbd37053613d1b9837af238b0cb381ebe9a85fc"},"cell_type":"code","source":"'''\nTrying to predict future stock prices as a classification problem.\n\nIn Technical Analysis people compute different indicators from historical data, like moving averages, rate of change, etc.\nMost indicators are only based on raw historical prices: open, close, low, high, volume.\nCrafting and combining those indicators is not straighforward. Typical questions that arise are:\n- Should I use simple moving average or exponential?\n- 20 days moving average or 50?\n- Is it important when the 20 days mov avg cross the 50 days or completely irrelevant?\n\nThe idea of this work is to let a neural network figure out which indicator should be created and how should be combined, without human intervention.\nEach traning sample is composed of N historical dates. For each date we have multiple features: open, close, high, low, volume.\nThe target is to predict the percentage increment of a stock price in M days in the future.\nEx: using a 260 days historical window predict if 60 days in the future the price will be higher than 10%.\nThe historical window can be seen as a time-series of 5 features for each slot.\nEach stock is splitted into multiple time-series. Training samples are a combination of time-series of multiple stocks all together.\n\nThere are multiple questions/variables that need to be defined to formulate this problem:\n- Length of historical window: 1 month, 6 months, 1 year, 5 years?\n- How far we should try to predict? 1 day, 1 month, 6 months, 2 years?\n- What should be our target/label? Price greater than 10% (binary), or multi-label (>10%, >20%, >30%)?\n- How to properly normalize features?\n- Network architecture for time-series analysis: MLP, LSTM, Conv1D?\n- Should we just use the raw date (open, close, etc) and let the network learn how to craft indicators or combine them?\n- Or should we pre-generate many indicators as features (SMA10, SMA20, SMA50, etc) and only let the network learn how to combine them?\n\nThe following work is testing one possible formulation of this problem:\n- Historical window of 1 year (260 business days)\n- Predicting price increase in the next 60 days\n- As a multi-label classification problem: price < -10% (10% loss or more), -10% < price < 10% (Flat), price > 10% (10% gain or more)\n- 500 stocks and a few ETFs are picked with Kaggle dataset\n- Samples are created with a rolling window of 5 days\n- Using a Convolutional 1D network architecture\n\nPlease be advised that results are still far from being accurate enough for real investment.\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport keras\nfrom keras import backend as K\nimport datetime as dt\nfrom IPython.display import display, HTML\nfrom sklearn.model_selection import train_test_split\nimport os\n\nRAND_SEED = 1\nnp.random.seed(RAND_SEED)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\"\"\"\nFunctions to load data\n\"\"\"\n\n# Pick first N symbols\ndef generate_stock_list(path, max_symbols):\n    all_stocks = np.array([f.split(\".\")[0] for f in os.listdir(path)])\n    return all_stocks[np.random.choice(len(all_stocks), max_symbols)]\n\n# Stocks and ETFs we'll be using\nBASE_PATH = \"../input/Data/\"\n#STOCKS = [\"tsla\", \"amzn\", \"fb\", \"aapl\", \"nvda\", \"goog\", \"nflx\", \"baba\", \"amd\", \"wmt\", \"bidu\", \"snap\", \"ibm\", \"f\", \"gm\", \"fcau\", \"twtr\", \"pzza\"]\nSTOCKS = generate_stock_list(BASE_PATH + \"Stocks/\", 500) #Increase this number to have more samples\nETFS = [\"spy\", \"qqq\", \"eem\", \"xlf\", \"gdx\", \"vxx\", \"ewz\", \"uso\", \"xlk\", \"hyg\", \"iyr\", \"tlt\", \"lqd\", \"agg\", \"ief\", \"veu\", \"fxi\"]\n\ndef load_symbol_df(dir_path, file_name, out_array):\n    full_path = dir_path + file_name\n    if os.stat(full_path).st_size > 0: \n        df = pd.read_csv(full_path)\n        columns = list(df.columns.values)\n        symbol = file_name.split(\".\")[0]\n        df[\"Symbol\"] = symbol\n        columns.insert(0, \"Symbol\")\n        out_array.append(df[columns])\n        \ndef load_data():\n    print(\"Loading {} symbols\".format(len(STOCKS) + len(ETFS)))\n    all_files_df = []\n    [load_symbol_df(BASE_PATH + \"Stocks/\", name + \".us.txt\", all_files_df) for name in STOCKS]\n    [load_symbol_df(BASE_PATH + \"ETFs/\", name + \".us.txt\", all_files_df) for name in ETFS]\n    merged_df = pd.concat(all_files_df)\n    all_dates = merged_df[\"Date\"].unique()\n    print(\"Total symbols loaded: \", len(merged_df[\"Symbol\"].unique()))\n    print(\"Dates from: {}, to: {}, total: {}\".format(all_dates.min(), all_dates.max(), len(all_dates)))\n    print(\"DataFrame size: \", len(merged_df))\n    return merged_df\n    \ndf = load_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92b1b395aead178b00065a42d23549f469756e95"},"cell_type":"code","source":"\"\"\"\nFunctions to convert data into samples\n\"\"\"\nFEATURE_VEC = [\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]\n\n# Create samples of a given size and with the given labels\ndef create_samples(df, windowLength, predDaysForward, labels, skipDays):\n    X = []\n    Y = []\n    symbols = df[\"Symbol\"].unique()\n    totalSamples = 0\n    for symbol in symbols:\n        symbol_df = df[df[\"Symbol\"] == symbol]\n        avail_days = len(symbol_df) - windowLength + 1 - predDaysForward\n        startIdx = 0\n        while startIdx < avail_days:\n            X.append(to_X(symbol_df, startIdx, windowLength))\n            Y.append(to_Y(symbol_df, startIdx, windowLength, predDaysForward, labels))\n            startIdx += skipDays\n            totalSamples += 1\n    return (np.array(X).reshape((totalSamples, windowLength, len(FEATURE_VEC))), \n            np.array(Y).reshape((totalSamples, len(labels))))\n \n# Create feture vec\ndef to_X(df, startIdx, windowLength):\n    X = []\n    winEndIdx = startIdx + windowLength\n    for item in FEATURE_VEC:\n        values = df[item].values\n        X.append(to_percentage(values[startIdx:winEndIdx], values[winEndIdx-1]))\n    return X\n\n# Create label vec\ndef to_Y(df, startIdx, windowLength, predDaysForward, labels):\n    prices = df[\"Close\"].values\n    ret = to_percentage(prices[startIdx + windowLength -1], prices[startIdx + windowLength -1 + predDaysForward]) * 100.0\n    label_vec = np.zeros(len(labels), dtype=\"float32\")\n    for label in labels:\n        if ret >= label[\"from\"] and ret < label[\"to\"]:\n            label_vec[label[\"label\"]] = 1.0\n            break\n    return label_vec\n    \n# Make values relative to a reference value\ndef to_percentage(values, base_value):\n    base_value = max(base_value, 0.001)\n    return ((values - base_value) / base_value).astype(\"float32\")\n\ndef shuffle_XY(X, Y):\n    shuffleIndexes = np.arange(Y.shape[0])\n    np.random.shuffle(shuffleIndexes)\n    return (X[shuffleIndexes], Y[shuffleIndexes])\n\n# Split into train and test set with a balanced number of samples for each class\ndef balance_samples(X, Y, labels, trainSetProportion):\n    # Find class with minimum number of samples\n    print(\"X shape: {}, Y shape: {}\".format(X.shape, Y.shape))\n    Y_num = np.argmax(Y, axis=1)\n    indices_per_label = [np.nonzero(Y_num == label)[0] for label in labels]\n    tot_per_label = np.array([len(indices) for indices in indices_per_label])\n    min_class = tot_per_label.min()\n    print(\"Total samples: {}, Min samples per class: {}. Count per class: {}\".format(len(Y), min_class, tot_per_label))\n    indices_per_label = [indices[np.random.choice(len(indices), min_class)] for indices in indices_per_label]\n    \n    # Split all samples per class by train/test\n    samples_per_label_split = []\n    tot_train_samples = 0\n    tot_test_samples = 0\n    for label in labels:\n        X_label = X[indices_per_label[label]]\n        Y_label = Y[indices_per_label[label]]\n        X_train, X_test, Y_train, Y_test = train_test_split(X_label, Y_label, test_size=1-trainSetProportion, random_state=RAND_SEED)\n        tot_train_samples += len(Y_train)\n        tot_test_samples += len(Y_test)\n        samples_per_label_split.append((X_train, X_test, Y_train, Y_test))\n    \n    # Append all\n    X_train = np.concatenate([s[0] for s in samples_per_label_split])\n    X_test = np.concatenate([s[1] for s in samples_per_label_split])\n    Y_train = np.concatenate([s[2] for s in samples_per_label_split])\n    Y_test = np.concatenate([s[3] for s in samples_per_label_split])\n    \n    # Reshuffle\n    X_train, Y_train = shuffle_XY(X_train, Y_train)\n    X_test, Y_test = shuffle_XY(X_test, Y_test)\n    \n    print(\"Train samples: {}, Test samples: {}\".format(len(Y_train), len(Y_test)))\n    return (X_train, X_test, Y_train, Y_test)\n    \ndef create_and_balance_samples(df, windowLength, predDaysForward, labels, trainSetProportion, skipDays):\n    X, Y = create_samples(df, windowLength, predDaysForward, labels, skipDays)\n    labels_num = [label[\"label\"] for label in labels]\n    return balance_samples(X, Y, labels_num, trainSetProportion)\n\ndef plot_training_progress(progress):\n    # Get data\n    y_loss = [i[\"loss\"] for i in progress]\n    y_val_loss = [i[\"val_loss\"] for i in progress]\n    y_acc = [i[\"categorical_accuracy\"] for i in progress]\n    y_val_acc = [i[\"val_categorical_accuracy\"] for i in progress]\n    x = np.arange(len(y_loss))\n    \n    #Plot loss\n    fig, ax = plt.subplots(1,1, figsize=(20,8))\n    plt.scatter(x, y_loss, c=\"red\", label=\"Train loss\")\n    plt.plot(x, y_loss, color=\"red\")\n    plt.scatter(x, y_val_loss, c=\"blue\", label=\"Val loss\")\n    plt.plot(x, y_val_loss, color=\"blue\")\n    plt.title(\"Loss progress\", fontsize=16)\n    plt.xlabel(\"Epochs\", fontsize=12)\n    plt.ylabel(\"Loss\", fontsize=12)\n    plt.legend()\n    plt.show()\n    \n    #Plot accuracy\n    fig, ax = plt.subplots(1,1, figsize=(20,8))\n    plt.scatter(x, y_acc, c=\"red\", label=\"Train accuracy\")\n    plt.plot(x, y_acc, color=\"red\")\n    plt.scatter(x, y_val_acc, c=\"blue\", label=\"Val accuracy\")\n    plt.plot(x, y_val_acc, color=\"blue\")\n    plt.title(\"Accuracy progress\", fontsize=16)\n    plt.xlabel(\"Epochs\", fontsize=12)\n    plt.ylabel(\"Accuracy\", fontsize=12)\n    plt.legend()\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59b827055b9d2a65a12042cc985ae339c57b6e79"},"cell_type":"code","source":"'''\nDefine network architecture\n'''\n\n# Conv1D model\ndef create_conv1D_model(windowLength, predDaysForward, labels):\n    model = keras.models.Sequential([\n        keras.layers.Conv1D(32, 3, activation='relu', input_shape=(windowLength, len(FEATURE_VEC))),\n        keras.layers.Conv1D(32, 3, activation='relu'),\n        keras.layers.MaxPooling1D(3),\n        keras.layers.Conv1D(64, 3, activation='relu'),\n        keras.layers.Conv1D(64, 3, activation='relu'),\n        keras.layers.MaxPooling1D(3),\n        keras.layers.Conv1D(128, 3, activation='relu'),\n        keras.layers.Conv1D(128, 3, activation='relu'),\n        keras.layers.Conv1D(128, 3, activation='relu'),\n        keras.layers.MaxPooling1D(3),\n        keras.layers.Flatten(),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(len(labels), activation=\"softmax\")\n    ])\n    return (\"Conv1D\", model)\n        \n# MLP model\ndef create_simple_dense_model(windowLength, predDaysForward, labels):\n    model = keras.models.Sequential([\n        keras.layers.Flatten(input_shape=(windowLength, len(FEATURE_VEC))),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(len(labels), activation=\"softmax\")\n    ])\n    return (\"SimpleDense\", model)\n\n# LSTM model\ndef create_LSTM_model(windowLength, predDaysForward, labels):\n    model = keras.models.Sequential([\n        keras.layers.LSTM(32, input_shape=(windowLength, len(FEATURE_VEC))),\n        keras.layers.Dense(len(labels), activation=\"softmax\")\n    ])\n    return (\"LSTM\", model)\n    \n# Conv1D + LSTM model\ndef create_conv1D_LSTM_model(windowLength, predDaysForward, labels):\n    model = keras.models.Sequential([\n        keras.layers.Conv1D(32, 5, activation='relu', input_shape=(windowLength, len(FEATURE_VEC))),\n        keras.layers.Conv1D(32, 5, activation='relu'),\n        keras.layers.MaxPooling1D(5),\n        keras.layers.Conv1D(64, 5, activation='relu'),\n        keras.layers.Conv1D(64, 5, activation='relu'),\n        keras.layers.MaxPooling1D(5),\n        keras.layers.LSTM(32, return_sequences=True, recurrent_dropout=0.5),\n        keras.layers.LSTM(32, recurrent_dropout=0.5),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(len(labels), activation=\"softmax\")\n    ])\n    return (\"Conv1D-LSTM\", model)\n    \n# Create model\ndef train_and_eval_model(X_train, X_test, Y_train, Y_test, windowLength, predDaysForward, labels, createModelFunc, \n                         epoch_rounds, log_steps, load_weights=False):\n    # Create Keras model\n    model_name, model = createModelFunc(windowLength, predDaysForward, labels)\n    print(\"Model: \", model_name)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.categorical_accuracy])\n    model.summary()\n    \n    # Callbacks to print status\n    progress = []\n    def print_progress(epoch, logs):\n        progress.append(logs)\n        if epoch % log_steps == 0:\n            print(\"\\tEpoch {}, Loss Train-Val: {} - {}, Accuracy Train-Val: {} - {}\".format(\n                epoch, round(logs[\"loss\"],6), round(logs[\"val_loss\"],6), round(logs[\"loss\"],6),\n                round(logs[\"categorical_accuracy\"],6), round(logs[\"val_categorical_accuracy\"],6)\n            ))\n        \n    model_base_path = \"models/\"\n    if not os.path.exists(model_base_path):\n        os.makedirs(model_base_path)\n    model_path = model_base_path + \"stocksClassifMultiFeature_{}_history{}_forward{}\".format(model_name, windowLength, predDaysForward)\n    fit_callbacks = [\n        keras.callbacks.ModelCheckpoint(model_path, monitor=\"loss\", save_weights_only=True),\n        keras.callbacks.LambdaCallback(on_epoch_end = lambda epoch, logs: print_progress(epoch, logs))\n    ]\n    \n    # Load existing weights\n    if load_weights:\n        print(\"Loading pre-trained weights from: \", model_path)\n        model.load_weights(model_path)\n    \n    # Train model, plot status after each round\n    BATCH_SIZE = 64\n    for r, epochs in enumerate(epoch_rounds):\n        print(\"Training round: {} with {} epochs\".format(r, epochs))\n        model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=epochs, verbose=0, callbacks=fit_callbacks, validation_data=(X_test, Y_test))\n        plot_training_progress(progress)\n    model.save_weights(model_path)\n    print(\"Training done\")\n    \n    # Get predictions\n    print(\"Predicting...\")\n    score_train = model.evaluate(X_train, Y_train, verbose=0)\n    score_test = model.evaluate(X_test, Y_test, verbose=0)\n    print(\"Train set - Loss: {}, Accuracy: {}%\".format(round(score_train[0],4), round(score_train[1]*100.0,4)))\n    print(\"Test set - Loss: {}, Accuracy: {}%\".format(round(score_test[0],4), round(score_test[1]*100.0,4)))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c9e623804f433eba83d2c69b5a1ef7605c77ec1"},"cell_type":"code","source":"# Create samples\nWINDOW_SIZE = 260 # 1 year of historical data (change this to increase/decrease window)\nFORWARD_DAYS = 60 # predict 3 months in the future\nSKIP_DAYS = 5 # when creating samples how many days we skip from window to window\nTRAIN_TEST_SPLIT = 0.8 # 80% samples for train, 20% for test\nLABELS = [ # Multi-label problem\n    {\"label\": 0, \"name\": \"10% loss\", \"from\": -99999999.0, \"to\": -10.0, \"return\": -0.1},\n    {\"label\": 1, \"name\": \"Flat\", \"from\": -10, \"to\": 10, \"return\": 0},\n    {\"label\": 2, \"name\": \"10% return\", \"from\": 10, \"to\": 99999999.0, \"return\": 0.1}\n]\n\nX_train, X_test, Y_train, Y_test = create_and_balance_samples(df, WINDOW_SIZE, FORWARD_DAYS, LABELS, TRAIN_TEST_SPLIT, SKIP_DAYS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"10c1e985973f864f7fbe294084683b22c6253bd1"},"cell_type":"code","source":"# Train model\nEPOCH_ROUNDS = [5] # Rounds of epochs to be used, change to something like this [10, 10, 5, 5] for better training\nmodel = train_and_eval_model(X_train, X_test, Y_train, Y_test, WINDOW_SIZE, FORWARD_DAYS, LABELS, create_conv1D_model,\n                             EPOCH_ROUNDS, log_steps=1, load_weights=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}